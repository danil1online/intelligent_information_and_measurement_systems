# –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ —É–∫–∞–∑–∞–Ω–∏—è –∫ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ ‚Ññ1 
**–¢–µ–º–∞:** –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞. 2 —á–∞—Å–∞

---

## üéØ –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã  
–û—Å–≤–æ–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ –º–µ—Ç–æ–¥—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö –æ–±—â–µ–¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤.

---

## üìå –ó–∞–¥–∞—á–∏  
- –ò–∑—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ Accuracy, Precision, Recall, F1-Score –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ 20 Newsgroups.
- –ò–∑—É—á–∏—Ç—å macro-/micro-averaging –Ω–∞ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ 20 Newsgroups.
- –ò–∑—É—á–∏—Ç—å Cosine, Jaccard –∏ Euclidean –º–µ—Ä—ã —Å—Ö–æ–¥—Å—Ç–≤–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ STS-Benchmark (Semantic Textual Similarity).
- –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ BERT/RoBERTa –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å TF-IDF.
- –í–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –º–µ—Ç–æ–¥–æ–º t-SNE –∏ UMAP.

---

## üìÅ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –º–µ—Ç–æ–¥—ã
- –Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äì Python 3.10.
- –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:
  - [matplotlib](https://matplotlib.org/),
  - [PyTorch (torch, torchtext)](https://pytorch.org/),
  - [datasets (Hugging Face)](https://huggingface.co/docs/datasets/index),
  - [transformers](https://huggingface.co/docs/transformers/index),
  - [seaborn](https://seaborn.pydata.org/)
  - [umap-learn](https://github.com/lmcinnes/umap).
- –î–∞—Ç–∞—Å–µ—Ç—ã
  - [20 Newsgroups (sklearn):](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html): –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ `alt.atheism` vs `soc.religion.christian`; –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è: –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ `alt.atheism`, `comp.graphics`, `sci.med`, `soc.religion.christian`
–∏–ª–∏
  - STS-Benchmark (Hugging Face ‚Äúglue‚Äù, ‚Äústsb‚Äù): –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0‚Äì5).

---

## üìö –ö—Ä–∞—Ç–∫–∞—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è  

### üìö –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

–í –∑–∞–¥–∞—á–µ –±–∏–Ω–∞—Ä–Ω–æ–π –∏–ª–∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:

1. üìö –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è

1.1. üìö Accuracy ‚Äî –¥–æ–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤:

$$
  Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
$$

1.2. üìö Precision ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:

$$
  Precision = \frac{TP}{TP + FP}
$$

1.3. üìö Recall ‚Äî –ø–æ–ª–Ω–æ—Ç–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤:

$$
  Recall = \frac{TP}{TP + FN}
$$

1.4. üìö F1-Score ‚Äî –≥–∞—Ä–º–æ–Ω–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ Precision –∏ Recall:

$$
  F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
$$

–ó–¥–µ—Å—å TP, TN, FP, FN ‚Äî —á–∏—Å–ª–∞ –∏—Å—Ç–∏–Ω–Ω–æ/–ª–æ–∂–Ω–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π.

2. üìö Macro- –∏ Micro-Averaging

–ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã—Ö –∑–∞–¥–∞—á–∞—Ö.

2.1. üìö Micro-Averaging

- –°—É–º–º–∏—Ä—É–µ—Ç –≤—Å–µ TP, FP, FN –ø–æ –∫–ª–∞—Å—Å–∞–º, –∑–∞—Ç–µ–º –≤—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫—É.
- –§–æ—Ä–º—É–ª–∞ –¥–ª—è Precision:

$$
  Precision_{micro} = \frac{\sum TP}{\sum TP + \sum FP}
$$

–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥–ª—è Recall –∏ F1.

2.2. üìö Macro-Averaging

- –í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫—É **–ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É**, –∑–∞—Ç–µ–º —É—Å—Ä–µ–¥–Ω—è–µ—Ç.
- –§–æ—Ä–º—É–ª–∞ –¥–ª—è Precision:

$$
  Precision_{macro} = \frac {1}{N} \sum_{i=1}^N Precision_i
$$



### üìö C—Ö–æ–¥—Å—Ç–≤–æ

1. üìö Cosine Similarity –∏–∑–º–µ—Ä—è–µ—Ç —É–≥–æ–ª –º–µ–∂–¥—É –¥–≤—É–º—è –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ —Ç–µ–∫—Å—Ç–æ–≤. –î–ª—è –¥–≤—É—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ ùê¥ –∏ ùêµ:

$$
  CosSim(A,B) = \frac{A \cdot B}{\Vert A \Vert \cdot \Vert B \Vert}
$$

–ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –æ—Ç ‚àí1 –¥–æ +1, –≥–¥–µ +1 ‚Äî –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π.

2. üìö Jaccard Similarity

- –ú–µ—Ä–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –º–Ω–æ–∂–µ—Å—Ç–≤–∞–º–∏:

$$
  J(A,B) = \frac{|A \cap B|}{|A \cup B|}
$$

- Jaccard Distance:

$$
  D_J(A,B) = 1 - J(A,B)
$$

3. üìö Euclidean Distance

- –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è –º–µ—Ç—Ä–∏–∫–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è:

$$
  d(p,q) = \sqrt {\sum_{i=1}^n (p_i - q_i)^2}
$$

- –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, KNN, PCA –∏ –¥—Ä.

### üìö BERT –∏ RoBERTa

–û–±–µ –º–æ–¥–µ–ª–∏ ‚Äî —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞.

1. üìö BERT (Bidirectional Encoder Representations from Transformers)
- –û–±—É—á–∞–µ—Ç—Å—è –Ω–∞:
  - [Masked Language Modeling (MLM)](https://huggingface.co/docs/transformers/tasks/masked_language_modeling)
  - [Next Sentence Prediction (NSP)](https://arxiv.org/abs/2109.03564)
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: —ç–Ω–∫–æ–¥–µ—Ä —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –≤–Ω–∏–º–∞–Ω–∏–µ–º.

2. üìö RoBERTa
- –£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è BERT:
  - –£–¥–∞–ª—ë–Ω NSP
  - –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –∏ —ç–ø–æ—Ö
  - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ç–∞ –∂–µ, –Ω–æ –æ–±—É—á–µ–Ω–∏–µ –±–æ–ª–µ–µ "–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ".

### üìö TF-IDF (Term Frequency‚ÄìInverse Document Frequency)

–ú–µ—Ç–æ–¥ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø–æ–∏—Å–∫–µ, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.

1. üìö Term Frequency (TF):

$$
  TF(t,d) = \frac {f_{t,d}}{\sum_{t'} f_{t',d}}
$$

2. üìö Inverse Document Frequency (IDF):

$$
  IDF(t) = \log \frac {N}{1 + df_t}
$$

3. üìö TF-IDF:

$$
  TFIDF(t,d) = TF(t,d) \times IDF(t)
$$

### üìö t-SNE –∏ UMAP

–ú–µ—Ç–æ–¥—ã —Å–Ω–∏–∂–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –æ—Å–æ–±–µ–Ω–Ω–æ —à–∏—Ä–æ–∫–æ–≤ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏. UMAP –±—ã—Å—Ç—Ä–µ–µ –∏ –ª—É—á—à–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è, —á–µ–º t-SNE.

1. üìö t-SNE (t-distributed Stochastic Neighbor Embedding)
- –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö.
- –®–∞–≥–∏:
  - –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π —Å—Ö–æ–¥—Å—Ç–≤–∞ –≤ –≤—ã—Å–æ–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ
  - –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ—á–µ–∫ –≤ 2D/3D
  - –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è KL-–¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –º–µ–∂–¥—É —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏

2. üìö UMAP (Uniform Manifold Approximation and Projection)
- –û—Å–Ω–æ–≤–∞–Ω –Ω–∞ —Ç–µ–æ—Ä–∏–∏ —Ç–æ–ø–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–π.
- –®–∞–≥–∏:
  - –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —Å–æ—Å–µ–¥–µ–π
  - –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ "fuzzy" —Ç–æ–ø–æ–ª–æ–≥–∏–∏
  - –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤ –Ω–∏–∑–∫–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ —á–µ—Ä–µ–∑ SGD


---
 
## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ä–µ–¥—ã

0. –ü–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ [Jupyter-Hub-–ò–ò–°–¢-–ù–ü–ò](http://195.133.13.56:8000/) –∏–∑ [–ø–µ—Ä–≤–æ–π —Ä–∞–±–æ—Ç—ã](docs/lab_1_cv_metrics.md#%EF%B8%8F-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-—Å—Ä–µ–¥—ã)
1. –°–æ–∑–¥–∞–π—Ç–µ –≤ –∫–æ—Ä–Ω–µ –¥–æ–º–∞—à–Ω–µ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –Ω–µ–≥–æ:
```bash

mkdir text_lab && cd text_lab
mkdir results

```
2. –°–æ–∑–¥–∞–π—Ç–µ –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
```bash

python3.10 -m venv venv
source venv/bin/activate

```

3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```bash

pip install --upgrade pip setuptools wheel
pip install torch torchtext --index-url https://download.pytorch.org/whl/cpu
pip install scikit-learn matplotlib transformers datasets umap-learn pandas seaborn


```

4. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

```python

#!/usr/bin/env python3
# file: download_data.py

import os
import pandas as pd
from sklearn.datasets import fetch_20newsgroups
from datasets import load_dataset

os.makedirs('data', exist_ok=True)

# 1. 20 Newsgroups: –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
cats_bin = ['alt.atheism', 'soc.religion.christian']
train_bin = fetch_20newsgroups(subset='train', categories=cats_bin,
                              remove=('headers','footers','quotes'))
test_bin  = fetch_20newsgroups(subset='test',  categories=cats_bin,
                              remove=('headers','footers','quotes'))

pd.DataFrame({'text': train_bin.data, 'label': train_bin.target}) \
  .to_csv('data/news_train_binary.csv', index=False)
pd.DataFrame({'text': test_bin.data,  'label': test_bin.target}) \
  .to_csv('data/news_test_binary.csv', index=False)

# 2. 20 Newsgroups: –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
cats_multi = ['alt.atheism','comp.graphics','sci.med','soc.religion.christian']
train_m = fetch_20newsgroups(subset='train', categories=cats_multi,
                            remove=('headers','footers','quotes'))
test_m  = fetch_20newsgroups(subset='test',  categories=cats_multi,
                            remove=('headers','footers','quotes'))

pd.DataFrame({'text': train_m.data, 'label': train_m.target}) \
  .to_csv('data/news_train_multi.csv', index=False)
pd.DataFrame({'text': test_m.data,  'label': test_m.target}) \
  .to_csv('data/news_test_multi.csv', index=False)

# 3. STS-Benchmark: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
sts = load_dataset('glue', 'stsb')
for split in ['train','validation','test']:
    df = pd.DataFrame({
        'sentence1': sts[split]['sentence1'],
        'sentence2': sts[split]['sentence2'],
        'score':     sts[split]['label']
    })
    df.to_csv(f'data/stsb_{split}.csv', index=False)

print("Datasets downloaded to ./data/")


```

---

## üß™ –ü—Ä–∏–º–µ—Ä—ã

### üß™ –ë–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è (20 Newsgroups)

**–ü–ª—é—Å—ã**: –±—ã—Å—Ç—Ä–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞, –ø–æ–Ω—è—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏. 
**–ú–∏–Ω—É—Å—ã**: —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞ –∫ –∫–∞—á–µ—Å—Ç–≤—É TF-IDF, —Å–ª–æ–∂–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –Ω–µ—Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã.

```python
#!/usr/bin/env python3
# file: classify_binary.py

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# –ó–∞–≥—Ä—É–∑–∫–∞
train = pd.read_csv('data/news_train_binary.csv')
test  = pd.read_csv('data/news_test_binary.csv')

# –£–¥–∞–ª–µ–Ω–∏–µ NaN –∏ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
train = train.dropna(subset=['text'])
test = test.dropna(subset=['text'])
train = train[train.text.str.strip().astype(bool)]
test = test[test.text.str.strip().astype(bool)]

vect = TfidfVectorizer(max_features=2000)
X_train = vect.fit_transform(train.text)
X_test  = vect.transform(test.text)
y_train = train.label
y_test  = test.label

# –û–±—É—á–µ–Ω–∏–µ
clf = LogisticRegression(max_iter=300)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)
print(f"Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
plt.bar(['Acc','Prec','Rec','F1'], [acc,prec,rec,f1], color=['#4C72B0','#55A868','#C44E52','#8172B3'])
plt.ylim(0,1); plt.title('Binary Classification Metrics')
plt.savefig('results/binary_metrics.png')

```


–ó–∞–ø—É—Å–∫ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏:

```bash
python classify_binary.py
```

### üß™ –ü—Ä–∏–º–µ—Ä 2. –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å macro/micro

**–ü–ª—é—Å—ã**: –º–∏–∫—Ä–æ- –∏ –º–∞–∫—Ä–æ-—Å—Ä–µ–¥–Ω–µ–µ –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤—ã –ø—Ä–∏ –¥–∏—Å–±–∞–ª–∞–Ω—Å–µ –∫–ª–∞—Å—Å–æ–≤. 
**–ú–∏–Ω—É—Å—ã**: macro –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É –∫–ª–∞—Å—Å–æ–≤, micro –∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–∞—Ö.

```python

#!/usr/bin/env python3
# file: classify_multi.py

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import precision_score, recall_score, f1_score
import matplotlib.pyplot as plt

# –ó–∞–≥—Ä—É–∑–∫–∞
train = pd.read_csv('data/news_train_multi.csv')
test  = pd.read_csv('data/news_test_multi.csv')

# –£–¥–∞–ª–µ–Ω–∏–µ NaN –∏ –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
train = train.dropna(subset=['text'])
test = test.dropna(subset=['text'])
train = train[train.text.str.strip().astype(bool)]
test = test[test.text.str.strip().astype(bool)]

vect = TfidfVectorizer(max_features=3000)
X_train = vect.fit_transform(train.text)
X_test  = vect.transform(test.text)
y_train = train.label
y_test  = test.label

# –ú–æ–¥–µ–ª—å
model = SVC(kernel='linear', probability=True)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# –ú–µ—Ç—Ä–∏–∫–∏
metrics = {
    'Precision (micro)': precision_score(y_test, y_pred, average='micro'),
    'Recall (micro)':    recall_score(y_test, y_pred, average='micro'),
    'F1 (micro)':        f1_score(y_test, y_pred, average='micro'),
    'Precision (macro)': precision_score(y_test, y_pred, average='macro'),
    'Recall (macro)':    recall_score(y_test, y_pred, average='macro'),
    'F1 (macro)':        f1_score(y_test, y_pred, average='macro'),
}

for name, val in metrics.items():
    print(f"{name}: {val:.3f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.bar(metrics.keys(), metrics.values(), color=plt.cm.tab20.colors)
plt.xticks(rotation=45, ha='right'); plt.ylim(0,1)
plt.title('Multi-Class micro/macro Metrics')
plt.tight_layout()
plt.savefig('results/multi_metrics.png')

```

–ó–∞–ø—É—Å–∫:

```bash

python classify_multi.py

```

### üß™ –ü—Ä–∏–º–µ—Ä 3. –ú–µ—Ä—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ (Jaccard, Euclidean, Cosine)

–ü–ª—é—Å—ã:

  - Jaccard –ø—Ä–æ—Å—Ç, –Ω–æ –¥–∏—Å–∫—Ä–µ—Ç–µ–Ω.
  - Euclidean —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∫ –¥–ª–∏–Ω–µ.
  - Cosine —É—Å—Ç–æ–π—á–∏–≤–∞ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é.

```python

#!/usr/bin/env python3
# file: semantic_measures.py

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import jaccard_score
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial.distance import euclidean
import matplotlib.pyplot as plt

# –ë–µ—Ä—ë–º –Ω–µ–±–æ–ª—å—à—É—é –≤—ã–±–æ—Ä–∫—É –∏–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ STS-B
df = pd.read_csv('data/stsb_validation.csv').sample(200, random_state=0)
s1 = df.sentence1.tolist()
s2 = df.sentence2.tolist()
gold = df.score.values

# TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
vect = TfidfVectorizer()
X1 = vect.fit_transform(s1).toarray()
X2 = vect.transform(s2).toarray()

# –í—ã—á–∏—Å–ª–µ–Ω–∏—è
jacc = [jaccard_score((X1[i]>0).astype(int), (X2[i]>0).astype(int)) for i in range(len(df))]
euc   = [euclidean(X1[i], X2[i]) for i in range(len(df))]
cos   = [cosine_similarity(X1[i:i+1], X2[i:i+1])[0,0] for i in range(len(df))]

# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è —Å –æ—Ü–µ–Ω–∫–æ–π —á–µ–ª–æ–≤–µ–∫–∞
print("Corr with gold:")
print(f" Jaccard vs score: {np.corrcoef(jacc, gold)[0,1]:.3f}")
print(f" Euclid vs score : {np.corrcoef(euc, gold)[0,1]:.3f}")
print(f" Cosine vs score : {np.corrcoef(cos, gold)[0,1]:.3f}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞: Cosine vs Gold
plt.scatter(gold, cos, alpha=0.5)
plt.xlabel('Human score'); plt.ylabel('Cosine sim'); plt.title('Cosine vs STS-B')
plt.savefig('results/semantic_cosine.png')
```

–ó–∞–ø—É—Å–∫:

```bash

python semantic_measures.py

```

### üß™ –ü—Ä–∏–º–µ—Ä 4. –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ Cosine Similarity

–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –≤ semantic_transformers.py.

```python

#!/usr/bin/env python3
# file: semantic_transformers.py

import pandas as pd
import numpy as np
import torch
from transformers import AutoModel, AutoTokenizer
from sklearn.metrics import mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model_name = 'bert-base-uncased'
tok   = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name).to(device)

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ä—ã
df = pd.read_csv('data/stsb_validation.csv').sample(200, random_state=1)
sent1 = df.sentence1.tolist()
sent2 = df.sentence2.tolist()
gold  = df.score.values

# –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è CLS-—ç–º–±–µ–¥–¥–∏–Ω–≥–∞
def embed(texts):
    enc = tok(texts, padding=True, truncation=True, return_tensors='pt').to(device)
    out = model(**enc, return_dict=True).last_hidden_state[:,0,:]
    return out.detach().cpu().numpy()

E1 = embed(sent1)
E2 = embed(sent2)
cos_t = [cosine_similarity(E1[i:i+1], E2[i:i+1])[0,0] for i in range(len(E1))]

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å TF-IDF Cosine
from sklearn.feature_extraction.text import TfidfVectorizer
vec = TfidfVectorizer().fit(sent1 + sent2)
T1 = vec.transform(sent1).toarray(); T2 = vec.transform(sent2).toarray()
cos_tf = [cosine_similarity(T1[i:i+1], T2[i:i+1])[0,0] for i in range(len(T1))]

print("MSE vs human:")
print(" BERT Cosine:", mean_squared_error(gold, cos_t))
print(" TF-IDF Cosine:", mean_squared_error(gold, cos_tf))

```

–ó–∞–ø—É—Å–∫:

```bash

python semantic_transformers.py

```

### üß™ –ü—Ä–∏–º–µ—Ä 5. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (t-SNE –∏ UMAP)

**–ü–ª—é—Å—ã**: –Ω–∞–≥–ª—è–¥–Ω–æ –≤—ã—è–≤–ª—è—é—Ç—Å—è –∫–ª–∞—Å—Ç–µ—Ä—ã –ø–æ —Ç–µ–º–∞–º. 
**–ú–∏–Ω—É—Å—ã**: t-SNE –º–µ–¥–ª–µ–Ω–Ω—ã–π, UMAP —Ç—Ä–µ–±—É–µ—Ç –ø–æ–¥–±–æ—Ä–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.

```python


#!/usr/bin/env python3
# file: visualize_embeddings.py

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
import umap
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# –î–∞–Ω–Ω—ã–µ: –Ω–µ–±–æ–ª—å—à–æ–π –ø–æ–¥–Ω–∞–±–æ—Ä 20 Newsgroups (–º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤—ã–π)
df = pd.read_csv('data/news_test_multi.csv').sample(400, random_state=2)
texts = df.text.tolist()
labels = df.label.tolist()

# –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ BERT
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model     = AutoModel.from_pretrained('bert-base-uncased').eval()
def get_embs(texts, labels):
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–∞—Ä (text, label)
    filtered = [(str(t), l) for t, l in zip(texts, labels) if isinstance(t, str) and t.strip()]
    texts_clean, labels_clean = zip(*filtered)

    enc = tokenizer(list(texts_clean), padding=True, truncation=True, return_tensors='pt')
    with torch.no_grad():
        out = model(**enc).last_hidden_state[:, 0, :]
    return out.numpy(), list(labels_clean)

E, labels = get_embs(texts, labels)

# t-SNE
tsne = TSNE(n_components=2, random_state=0)
E_tsne = tsne.fit_transform(E)

# UMAP
um = umap.UMAP(n_components=2, random_state=0)
E_umap = um.fit_transform(E)

assert E_tsne.shape[0] == len(labels), "–†–∞–∑–º–µ—Ä—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –º–µ—Ç–æ–∫ –Ω–µ —Å–æ–≤–ø–∞–¥–∞—é—Ç!"

# –†–∏—Å—É–µ–º
# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
print(f"E_tsne shape: {E_tsne.shape}")
print(f"labels length: {len(labels)}")

# –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ labels –∫ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω–µ
if len(labels) != E_tsne.shape[0]:
    print("‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤: –æ–±—Ä–µ–∑–∞–µ–º labels")
    labels = labels[:E_tsne.shape[0]]

# –°–æ–∑–¥–∞–Ω–∏–µ DataFrame
df_plot = pd.DataFrame({
    'x': E_tsne[:, 0],
    'y': E_tsne[:, 1],
    'label': labels
})

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
fig, axes = plt.subplots(1, 2, figsize=(14, 6))

# t-SNE scatterplot
sns.scatterplot(data=df_plot, x='x', y='y', hue='label', palette='tab10', s=20, ax=axes[0])
axes[0].set_title('t-SNE Embedding Visualization')
axes[0].legend(title='Class', bbox_to_anchor=(1.05, 1), loc='upper left')

# –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–ª–∞—Å—Å–æ–≤
sns.countplot(data=df_plot, x='label', hue='label', palette='tab10', ax=axes[1], legend=False)
axes[1].set_title('Label Distribution')
axes[1].set_xlabel('Class')
axes[1].set_ylabel('Count')

plt.tight_layout()
plt.savefig('results/embeddings_vis.png')
print("Result saved in results/embeddings_vis.png")

```

–ó–∞–ø—É—Å–∫:

```bash

python visualize_embeddings.py

```

---
### üìå –ó–∞–¥–∞–Ω–∏–µ –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã

1. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å ROC-–∫—Ä–∏–≤—É—é –∏ –≤—ã—á–∏—Å–ª–∏—Ç—å AUC –¥–ª—è –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏.
2. –°—Ä–∞–≤–Ω–∏—Ç—å SVM –∏ LogisticRegression –Ω–∞ –±–∏–Ω–∞—Ä–Ω–æ–º –∏ –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–º –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.
3. –ü—Ä–æ–≤–µ—Å—Ç–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Ç—Ä—ë—Ö –º–µ—Ä —Å—Ö–æ–¥—Å—Ç–≤–∞ (Jaccard, Euclidean, Cosine) —Å –æ—Ü–µ–Ω–∫–∞–º–∏ STS-B.
4. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ transformers (RoBERTa, DistilBERT) –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å MSE.
5. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ UMAP (n_neighbors, min_dist) –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é.

---

## üí° –ù–µ –∑–∞–±—É–¥—å—Ç–µ –≤—ã–∫–ª—é—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ä–µ–¥—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã python (–¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–ø–∞—Å—Ç—å –Ω–∞–¥–ø–∏—Å—å (venv) –≤ –Ω–∞—á–∞–ª–µ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏):

```bash

deactivate

```


## –í–æ–ø—Ä–æ—Å—ã
1. –ö–∞–∫ macro- –∏ micro-averaging –æ—Ç—Ä–∞–∂–∞—é—Ç –±–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –æ–±—â–∏–º –∫–∞—á–µ—Å—Ç–≤–æ–º –∏ –∫–∞—á–µ—Å—Ç–≤–æ–º –Ω–∞ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–∞—Ö?
2. –í –∫–∞–∫–∏—Ö —Å–ª—É—á–∞—è—Ö –ø—Ä–æ—Å—Ç—ã–µ –º–µ—Ä—ã (Jaccard, Euclidean) —É—Å—Ç—É–ø–∞—é—Ç Cosine –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º?
3. –ù–∞—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–∂–Ω–µ–µ –∏ —Ä–µ—Å—É—Ä—Å–æ—ë–º–∫–æ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å TF-IDF?
4. –ö–∞–∫–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø—Ä–∏—ë–º—ã –ø–æ–∑–≤–æ–ª—è—é—Ç –ª—É—á—à–µ –ø–æ–Ω—è—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö?
5. –ö–∞–∫ –≤—ã–±–æ—Ä –ø–æ—Ä–æ–≥–∞ –≤–ª–∏—è–µ—Ç –Ω–∞ Precision –∏ Recall –≤ –∑–∞–¥–∞—á–∞—Ö —Å –Ω–µ—Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –∫–ª–∞—Å—Å–∞–º–∏?
6. –ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ —É –∫–∞–∂–¥–æ–π –∏–∑ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏?
7. –ö–∞–∫ –≤–µ–ª–∏—á–∏–Ω–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞ —Å–æ–æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç—å—é –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤?
8. –ö–∞–∫–∏–µ –º–µ—Ç–æ–¥—ã –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –æ–∫–∞–∑–∞–ª–∏—Å—å –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º–∏ –¥–ª—è –≤–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö?

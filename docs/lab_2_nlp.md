# –ú–µ—Ç–æ–¥–∏—á–µ—Å–∫–∏–µ —É–∫–∞–∑–∞–Ω–∏—è –∫ –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–Ω–æ–π —Ä–∞–±–æ—Ç–µ ‚Ññ6 
**–¢–µ–º–∞:** –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞. –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

---

## üéØ –¶–µ–ª—å —Ä–∞–±–æ—Ç—ã  
–ü–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–∏—Å—Ç–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞: BLEU, ROUGE –∏ Perplexity.

---

## üìå –ó–∞–¥–∞—á–∏  
- –ò–∑—É—á–∏—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫–∏ BLEU.
- –ò–∑—É—á–∏—Ç—å –ø—Ä–∏–Ω—Ü–∏–ø—ã —Ä–∞—Å—á—ë—Ç–∞ –º–µ—Ç—Ä–∏–∫ ROUGE (ROUGE-N, ROUGE-L).
- –ò–∑—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫—É Perplexity –¥–ª—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

---

## üìÅ –ú–∞—Ç–µ—Ä–∏–∞–ª—ã –∏ –º–µ—Ç–æ–¥—ã
- –Ø–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è ‚Äì Python 3.10.
- –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:
  - [matplotlib](https://matplotlib.org/),
  - [scikit-learn](https://scikit-learn.org/),
  - [rouge-score (–¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ ROUGE)](https://github.com/google-research/google-research/tree/master/rouge)
  - [nltk (–¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ BLEU)](https://www.nltk.org/).
- –î–∞—Ç–∞—Å–µ—Ç
–î–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≤–æ–∑—å–º—ë–º –Ω–µ–±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å –∏–∑ 50 –ø–∞—Ä ¬´reference‚Äìhypothesis¬ª –∏–∑ –¥–∞–Ω–Ω—ã—Ö nltk. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –æ–¥–Ω—É —ç—Ç–∞–ª–æ–Ω–Ω—É—é –∏ –æ–¥–Ω—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º–æ–π —Ñ—Ä–∞–∑—É.

---

## üìö –ö—Ä–∞—Ç–∫–∞—è —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è  

–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç, —á—Ç–æ —Å–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞—ë—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ –≤—Ö–æ–¥–∞. –û—Ü–µ–Ω–∏—Ç—å –µ—ë –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–∂–Ω–æ —Å –ø–æ–º–æ—â—å—é –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è—é—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º (reference).

1. üìö BLEU (Bilingual Evaluation Understudy) –∏–∑–º–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å n-–≥—Ä–∞–º–º. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ–æ—Ä–º—É–ª–∞ –≤—ã–≥–ª—è–¥–∏—Ç —Ç–∞–∫:

$$
  BLUE = BP \cdot \exp(\sum_{n=1}^N w_n \log p_n)
$$

–≥–¥–µ ùëù_ùëõ ‚Äî —Ç–æ—á–Ω–æ—Å—Ç—å n-–≥—Ä–∞–º–º, ùë§_ùëõ ‚Äî –≤–µ—Å n-–≥—Ä–∞–º–º, BP ‚Äî —à—Ç—Ä–∞—Ñ –∑–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –≥–∏–ø–æ—Ç–µ–∑–Ω—ã–π —Ç–µ–∫—Å—Ç.

2. üìö ROUGE (Recall-Oriented Understudy for Gisting Evaluation) –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –ø–æ–ª–Ω–æ—Ç—É: ROUGE-N –¥–ª—è n-–≥—Ä–∞–º–º, ROUGE-L –¥–ª—è –ª–æ–Ω–≥–µ—Å—Ç –æ–±—â–µ–π –ø–æ–¥–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. –ù–∞–ø—Ä–∏–º–µ—Ä, ROUGE-N —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫

$$
  ROUGE_N = \frac{\sum_{gram_n \in Ref} min(count_{Ref}, count_{Hyp})}{\sum_{gram_n \in Ref} count_{Ref}}
$$

3. üìö Perplexity (–ø–µ—Ä–ø–ª–µ–∫—Å–∏—è) –∏–∑–º–µ—Ä—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç. –î–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ ùëä=ùë§_1‚Ä¶ùë§_ùëÅ:

$$
  Perplexity(W) = P(W)^{-1/N}
$$

–≥–¥–µ ùëÉ(ùëä) ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤—Å–µ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª—å—é.


---
 
## ‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ä–µ–¥—ã

0. –ü–æ–¥–∫–ª—é—á–∏—Ç–µ—Å—å –∫ [Jupyter-Hub-–ò–ò–°–¢-–ù–ü–ò](http://195.133.13.56:8000/) –∏–∑ [–ø–µ—Ä–≤–æ–π —Ä–∞–±–æ—Ç—ã](docs/lab_1_cv_metrics.md#%EF%B8%8F-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞-—Å—Ä–µ–¥—ã)
1. –°–æ–∑–¥–∞–π—Ç–µ –≤ –∫–æ—Ä–Ω–µ –¥–æ–º–∞—à–Ω–µ–≥–æ –∫–∞—Ç–∞–ª–æ–≥–∞ –∫–∞—Ç–∞–ª–æ–≥ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ –Ω–µ–≥–æ:
```bash

mkdir text_eval_lab && cd text_eval_lab
mkdir results data

```
2. –°–æ–∑–¥–∞–π—Ç–µ –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–π—Ç–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
```bash

python3.10 -m venv venv
source venv/bin/activate

```

3. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```bash

pip install --upgrade pip setuptools wheel
pip install scikit-learn matplotlib nltk rouge-score


```

4. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞—ë—Ç –≤ –ø–∞–ø–∫–µ data/ –¥–≤–∞ —Ñ–∞–π–ª–∞:
- `ref.txt` ‚Äî 50 —Å–ª—É—á–∞–π–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–∑ –∫–æ—Ä–ø—É—Å–∞ Brown;
- `hyp.txt` ‚Äî ¬´–∏—Å–∫–∞–∂—ë–Ω–Ω—ã–µ¬ª –≤–µ—Ä—Å–∏–∏ —Ç–µ—Ö –∂–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä + —É–¥–∞–ª—ë–Ω–Ω–æ–µ —Å–ª—É—á–∞–π–Ω–æ–µ —Å–ª–æ–≤–æ).

```python

#!/usr/bin/env python3
# file: prepare_data.py

import os
import random
from nltk.corpus import brown
from nltk import download

# 1. Ensure NLTK resources are available
download('brown', quiet=True)
download('punkt', quiet=True)

# 2. Create directories if missing
os.makedirs('data', exist_ok=True)
os.makedirs('results', exist_ok=True)

# 3. Sample 50 sentences from Brown Corpus
random.seed(42)
sentences = list(brown.sents())
sample = random.sample(sentences, 50)

# 4. Write reference sentences
with open('data/ref.txt', 'w', encoding='utf-8') as f_ref:
    for sent in sample:
        f_ref.write(' '.join(sent) + '\n')

# 5. Generate hypotheses by lowercasing and dropping one random word
def perturb(sentence: str) -> str:
    tokens = sentence.split()
    if len(tokens) > 3:
        idx = random.randrange(len(tokens))
        tokens.pop(idx)
    return ' '.join(tokens).lower()

with open('data/hyp.txt', 'w', encoding='utf-8') as f_hyp:
    for sent in sample:
        orig = ' '.join(sent)
        f_hyp.write(perturb(orig) + '\n')

print('–ö–æ—Ä–ø—É—Å –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω: data/ref.txt –∏ data/hyp.txt (50 —Å—Ç—Ä–æ–∫ –∫–∞–∂–¥–∞—è).')
print('–ö–∞—Ç–∞–ª–æ–≥–∏ data/ –∏ results/ —Å–æ–∑–¥–∞–Ω—ã.')

```

–ó–∞–ø—É—Å–∫ —Å–∫—Ä–∏–ø—Ç–∞

```bash

python prepare_data.py

```

–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –≤ `data/` –ø–æ—è–≤—è—Ç—Å—è –¥–≤–∞ —Ñ–∞–π–ª–∞ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏, –∞ –≤ –∫–æ—Ä–Ω–µ –µ—Å—Ç—å –ø—É—Å—Ç–æ–π –∫–∞—Ç–∞–ª–æ–≥ `results/` –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ –æ—Ç—á—ë—Ç–æ–≤.

---

## üß™ –ü—Ä–∏–º–µ—Ä—ã

### üß™ –û—Ü–µ–Ω–∫–∞ BLEU (—Ñ–∞–π–ª evaluate_bleu.py)

```python
#!/usr/bin/env python3
# file: evaluate_bleu.py

import sys
import nltk
from nltk.translate.bleu_score import corpus_bleu
from matplotlib import pyplot as plt

def load_sentences(path):
    return [line.strip().split() for line in open(path, encoding='utf-8')]

if __name__ == '__main__':
    ref_path, hyp_path, out_png = sys.argv[1], sys.argv[2], sys.argv[3]
    references = [[sent] for sent in load_sentences(ref_path)]
    hypotheses = load_sentences(hyp_path)
    bleu_score = corpus_bleu(references, hypotheses) * 100
    print(f'BLEU score: {bleu_score:.2f}')
    plt.bar(['BLEU'], [bleu_score])
    plt.ylim(0, 100)
    plt.savefig(out_png)

```


–ó–∞–ø—É—Å–∫ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏:

```bash
python evaluate_bleu.py data/ref.txt data/hyp.txt results/bleu.png
```

### üß™ –ü—Ä–∏–º–µ—Ä 2. –û—Ü–µ–Ω–∫–∞ ROUGE (—Ñ–∞–π–ª evaluate_rouge.py)

```python

#!/usr/bin/env python3
# file: evaluate_rouge.py

import sys
from rouge_score import rouge_scorer
from matplotlib import pyplot as plt

if __name__ == '__main__':
    ref_path, hyp_path, out_png = sys.argv[1], sys.argv[2], sys.argv[3]
    refs = open(ref_path, encoding='utf-8').read().splitlines()
    hyps = open(hyp_path, encoding='utf-8').read().splitlines()
    scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)
    scores = [scorer.score(r, h) for r, h in zip(refs, hyps)]
    avg_rouge1 = sum(s['rouge1'].fmeasure for s in scores) / len(scores) * 100
    avg_rougeL = sum(s['rougeL'].fmeasure for s in scores) / len(scores) * 100
    print(f'ROUGE-1: {avg_rouge1:.2f}, ROUGE-L: {avg_rougeL:.2f}')
    plt.bar(['ROUGE-1','ROUGE-L'], [avg_rouge1, avg_rougeL])
    plt.ylim(0,100)
    plt.savefig(out_png)

```

–ó–∞–ø—É—Å–∫:

```bash

python evaluate_rouge.py data/ref.txt data/hyp.txt results/rouge.png

```

### üß™ –ü—Ä–∏–º–µ—Ä 3. –û—Ü–µ–Ω–∫–∞ Perplexity (—Ñ–∞–π–ª evaluate_perplexity.py)

```python

#!/usr/bin/env python3
# file: evaluate_perplexity.py

import sys
import math
from collections import defaultdict
from nltk.tokenize import TreebankWordTokenizer
from matplotlib import pyplot as plt
import nltk

# üì¶ –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –Ω—É–∂–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã
nltk.download('punkt', quiet=True)

def build_ngram_counts(sentences, n=3):
    counts = defaultdict(int)
    total = 0
    tokenizer = TreebankWordTokenizer()
    for s in sentences:
        tokens = ['<s>'] * (n - 1) + tokenizer.tokenize(s.lower()) + ['</s>']
        for i in range(len(tokens) - n + 1):
            gram = tuple(tokens[i:i + n])
            counts[gram] += 1
            total += 1
    return counts, total

def perplexity(sentences, counts, total, n=3, alpha=1.0):
    pp_sum = 0
    N = 0
    tokenizer = TreebankWordTokenizer()
    for s in sentences:
        tokens = ['<s>'] * (n - 1) + tokenizer.tokenize(s.lower()) + ['</s>']
        for i in range(n - 1, len(tokens)):
            context = tuple(tokens[i - n + 1:i])
            gram = tuple(tokens[i - n + 1:i + 1])
            num = counts[gram] + alpha
            # üõ°Ô∏è –ó–∞—â–∏—Ç–∞ –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
            den = sum(counts[c] + alpha for c in counts if c[:-1] == context)
            prob = num / den if den > 0 else 1e-10
            pp_sum += -math.log(prob)
            N += 1
    return math.exp(pp_sum / N) if N > 0 else float('inf')

if __name__ == '__main__':
    if len(sys.argv) != 3:
        print("‚ùå Usage: python evaluate_perplexity.py <ref.txt> <output.png>")
        sys.exit(1)

    ref_path, out_png = sys.argv[1], sys.argv[2]

    try:
        with open(ref_path, encoding='utf-8') as f:
            refs = f.read().splitlines()
    except FileNotFoundError:
        print(f"‚ùå Error: File '{ref_path}' not found.")
        sys.exit(1)

    # üìä –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –ø–µ—Ä–ø–ª–µ–∫—Å–∏–∏
    counts, total = build_ngram_counts(refs, n=3)
    alphas = [0.1, 0.5, 1.0]
    pps = [perplexity(refs, counts, total, n=3, alpha=a) for a in alphas]

    # üñ®Ô∏è –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∫–æ–Ω—Å–æ–ª—å
    print("üìà Perplexity values:")
    for a, p in zip(alphas, pps):
        print(f"  alpha={a}: perplexity={p:.4f}")

    # üìâ –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
    plt.plot(alphas, pps, marker='o')
    plt.title('Trigram Perplexity vs. Smoothing')
    plt.xlabel('alpha (smoothing)')
    plt.ylabel('Perplexity')
    plt.grid(True)
    plt.savefig(out_png)
```

–ó–∞–ø—É—Å–∫:

```bash

python evaluate_perplexity.py data/ref.txt results/perplexity.png

```

---
### üìå –ó–∞–¥–∞–Ω–∏–µ –¥–ª—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã

1. –°—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–µ—Ç—Ä–∏–∫ BLEU –∏ ROUGE –Ω–∞ –Ω–∞–±–æ—Ä–µ –≥–∏–ø–æ—Ç–µ–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.
2. –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å ROUGE-2 –∏ ROUGE-LCS –≤—Ä—É—á–Ω—É—é –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è rouge-score.
3. –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç—å –≤–ª–∏—è–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∫–æ—Ä–ø—É—Å–∞ –Ω–∞ perplexity, –∏–∑–º–µ–Ω–∏–≤ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ data/ref.txt.

---

## üí° –ù–µ –∑–∞–±—É–¥—å—Ç–µ –≤—ã–∫–ª—é—á–∏—Ç—å —Ç–µ–∫—É—â—É—é —Å—Ä–µ–¥—É –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã python (–¥–æ–ª–∂–Ω–∞ –ø—Ä–æ–ø–∞—Å—Ç—å –Ω–∞–¥–ø–∏—Å—å (venv) –≤ –Ω–∞—á–∞–ª–µ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏):

```bash

deactivate

```


## –í–æ–ø—Ä–æ—Å—ã
1. –ö–∞–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç BLEU, –∞ –∫–∞–∫–∏–µ ROUGE?
2. –í –∫–∞–∫–∏—Ö —Å–∏—Ç—É–∞—Ü–∏—è—Ö –≤—ã—Å–æ–∫–∞—è –ø–µ—Ä–ø–ª–µ–∫—Å–∏—è –º–æ–∂–µ—Ç —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤–æ–≤–∞—Ç—å –æ –Ω–∏–∑–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–∏?
3. –ö–∞–∫–æ–≤–∞ —Ä–æ–ª—å –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –¥–ª–∏–Ω—É (BP) –≤ BLEU?
4. –ß–µ–º –ø–æ–ª–µ–∑–µ–Ω –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ perplexity –æ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–∞ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è?
5. –ö–∞–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫ —Å–æ–æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π —á–µ–ª–æ–≤–µ–∫–æ–º?
6. –ö–∞–∫–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —É BLEU –∏ ROUGE –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ —Ç–≤–æ—Ä—á–µ—Å–∫–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (—Å–µ–∫—Ä–µ—Ç—ã, –¥–∏–∞–ª–æ–≥–∏, –ø–æ—ç–∑–∏—è)?
